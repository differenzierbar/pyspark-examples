{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8283358-86af-4277-a86e-d6d12cdb13f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "k8s_host=os.environ.get('KUBERNETES_SERVICE_HOST')\n",
    "k8s_port=os.environ.get('KUBERNETES_SERVICE_PORT')\n",
    "\n",
    "sparkConf = SparkConf()\n",
    "sparkConf.setMaster(\"k8s://https://\" + k8s_host + \":\" + k8s_port)\n",
    "sparkConf.setAppName(\"KUBERNETES-IS-AWESOME\")\n",
    "sparkConf.set(\"spark.kubernetes.container.image\", \"apache/spark-py:v3.3.1\")\n",
    "sparkConf.set(\"spark.kubernetes.namespace\", \"spark\")\n",
    "sparkConf.set(\"spark.kubernetes.authenticate.driver.serviceAccountName\", \"spark\")\n",
    "sparkConf.set(\"spark.driver.host\", os.environ.get('POD_IP'))\n",
    "\n",
    "sparkConf.set(\"spark.kubernetes.executor.volumes.hostPath.work.mount.path\", \"/home/jovyan/work\")\n",
    "sparkConf.set(\"spark.kubernetes.executor.volumes.hostPath.work.options.path\", \"/home/tobias/ecc\")\n",
    "\n",
    "sparkConf.set(\"spark.executor.memory\", \"8g\")\n",
    "\n",
    "spark = SparkSession.builder.config(conf=sparkConf).getOrCreate()\n",
    "sc = spark.sparkContext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2bbbe7c3-126d-4dfa-bc5a-3ba8c4a1bd5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- index: integer (nullable = true)\n",
      " |-- member_id: integer (nullable = true)\n",
      " |-- contract_id: string (nullable = true)\n",
      " |-- value: string (nullable = true)\n",
      "\n",
      "+-----+---------+-----------+-----+\n",
      "|index|member_id|contract_id|value|\n",
      "+-----+---------+-----------+-----+\n",
      "|    0|        0|          0|   12|\n",
      "|    1|        0|          1|   17|\n",
      "|    2|        0|          2|  -16|\n",
      "|    3|        0|          3|   16|\n",
      "|    4|        0|          4|   -7|\n",
      "|    5|        0|          5|  -15|\n",
      "|    6|        0|          6|  -19|\n",
      "|    7|        0|          7|   -8|\n",
      "|    8|        0|          8|  -16|\n",
      "|    9|        0|          9|    4|\n",
      "|   10|        0|         10|  -10|\n",
      "|   11|        0|         11|   13|\n",
      "|   12|        0|         12|   11|\n",
      "|   13|        0|         13|   -3|\n",
      "|   14|        0|         14|    1|\n",
      "|   15|        0|         15|   -9|\n",
      "|   16|        0|         16|    5|\n",
      "|   17|        0|         17|   -4|\n",
      "|   18|        0|         18|   -5|\n",
      "|   19|        0|         19|    5|\n",
      "+-----+---------+-----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, IntegerType, StringType\n",
    "\n",
    "schema_positions = StructType() \\\n",
    "      .add(\"index\",IntegerType(),True) \\\n",
    "      .add(\"member_id\",IntegerType(),True) \\\n",
    "      .add(\"contract_id\",StringType(),True) \\\n",
    "      .add(\"value\",StringType(),True)\n",
    "\n",
    "#positions = spark.read.options(header='True', inferSchema='True').csv(\"../spark-examples/positions.csv\")\n",
    "#positions = spark.read.options(header='True', inferSchema='True', index_col='RecordNumber').schema(schema).csv(\"../spark-examples/positions.csv\")\n",
    "spark_df_positions = spark.read.options(header='True').schema(schema_positions).csv(\"../spark-examples/positions.csv\")\n",
    "spark_df_positions = spark_df_positions.drop('RecordNumber')\n",
    "\n",
    "spark_df_positions.printSchema()\n",
    "spark_df_positions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "968b980e-de61-4771-9da5-862d48c7f792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- scenario_Id: integer (nullable = true)\n",
      " |-- shift_value: string (nullable = true)\n",
      "\n",
      "+-----------+------------------+\n",
      "|scenario_Id|       shift_value|\n",
      "+-----------+------------------+\n",
      "|          0| 1.136747665263346|\n",
      "|          1|0.9970534238176361|\n",
      "|          2|1.2012950042542534|\n",
      "|          3| 1.068999218388157|\n",
      "|          4| 0.974422521640139|\n",
      "|          5|0.9169427110775529|\n",
      "|          6|1.0140460263340396|\n",
      "|          7| 0.998353822143433|\n",
      "|          8| 0.909461467928445|\n",
      "|          9| 1.009716422091574|\n",
      "|         10|1.1333038201545973|\n",
      "|         11|0.8969101265348353|\n",
      "|         12|0.9751994163676627|\n",
      "|         13|0.8953357581601943|\n",
      "|         14|1.0458845724152983|\n",
      "|         15|0.9788871005104854|\n",
      "|         16| 0.932349950373955|\n",
      "|         17| 1.006109170231296|\n",
      "|         18|0.9720614593702699|\n",
      "|         19|1.1110682511059442|\n",
      "+-----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema_scenarios = StructType() \\\n",
    "      .add(\"index\",IntegerType(),True) \\\n",
    "      .add(\"scenario_Id\",IntegerType(),True) \\\n",
    "      .add(\"shift_value\",StringType(),True)\n",
    "\n",
    "#positions = spark.read.options(header='True', inferSchema='True').csv(\"../spark-examples/positions.csv\")\n",
    "#positions = spark.read.options(header='True', inferSchema='True', index_col='RecordNumber').schema(schema).csv(\"../spark-examples/positions.csv\")\n",
    "spark_df_scenarios = spark.read.options(header='True').schema(schema_scenarios).csv(\"../spark-examples/scenarios.csv\")\n",
    "spark_df_scenarios = spark_df_scenarios.drop('index')\n",
    "\n",
    "spark_df_scenarios.printSchema()\n",
    "spark_df_scenarios.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a5532fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "joining...\n",
      "+---------+-----------+-------------------+\n",
      "|member_id|scenario_id|sum(pnl_value)     |\n",
      "+---------+-----------+-------------------+\n",
      "|0        |190        |-69.18207792913546 |\n",
      "|0        |632        |-69.16282119662804 |\n",
      "|0        |716        |-68.77338113182522 |\n",
      "|0        |915        |-61.98909991900546 |\n",
      "|0        |1218       |-64.85855314918105 |\n",
      "|0        |1237       |-67.53840526210047 |\n",
      "|0        |1265       |-64.3799491168693  |\n",
      "|0        |1327       |-64.38461719562324 |\n",
      "|0        |1478       |-67.14209073367797 |\n",
      "|0        |1578       |-62.911256205017196|\n",
      "|0        |1761       |-67.6904852378482  |\n",
      "|0        |1790       |-69.53589329345252 |\n",
      "|0        |1866       |-69.5958417819003  |\n",
      "|0        |2248       |-67.79524489233343 |\n",
      "|0        |2395       |-66.6113374681786  |\n",
      "|0        |2593       |-66.05047862806636 |\n",
      "|0        |3137       |-69.14397152083643 |\n",
      "|0        |3164       |-65.76604760952448 |\n",
      "|0        |3645       |-66.62506936813895 |\n",
      "|0        |3700       |-65.28099673088725 |\n",
      "+---------+-----------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # Logic\n",
    "# df_pnl = df_positions.merge(df_scenarios, how=\"cross\")\n",
    "# df_pnl[\"pnl_value\"] = df_pnl[\"value\"] * df_pnl[\"shift_value\"]\n",
    "# print(str(df_pnl.size))\n",
    "\n",
    "# df_pnl = df_pnl.groupby([\"member_id\", \"scenario_id\"], as_index=False)[\"pnl_value\"].sum() # Result\n",
    "# print(df_pnl)\n",
    "\n",
    "# print(\"done\")\n",
    "\n",
    "print(\"joining...\")\n",
    "joined = spark_df_positions.crossJoin(spark_df_scenarios)\n",
    "#result = joined.withColumn('pnl_value', joined.value - joined.shift_value).groupBy(\"member_id\", \"scenario_id\").sum(\"pnl_value\").collect()\n",
    "joined.withColumn('pnl_value', joined.value - joined.shift_value).groupBy(\"member_id\", \"scenario_id\").sum(\"pnl_value\").show(truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "463fe566-3fa9-432e-a31a-6834ed6c6360",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c140923c-03ee-4383-919e-b9a4786d93a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
